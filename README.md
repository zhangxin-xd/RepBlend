# üåü Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation
# NeurIPS 2025 (Rating: 4445)
> [Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation](https://arxiv.org/pdf/2505.14705?).<br>
> [Xin Zhang](https://zhangxin-xd.github.io/), Ziruo Zhang, [Jiawei Du](https://scholar.google.com/citations?user=WrJKEzEAAAAJ&hl=zh-CN), [Zuozhu Liu](https://person.zju.edu.cn/en/lzz), [Joey Tianyi Zhou](https://joeyzhouty.github.io/) <br>
> Agency for Science, Technology, and Research (ASTAR), Singapore <br>
> National University of Singapore, Singapore <br>
> Zhejiang University, China 
## üìñ Introduction
<p align="center">
  <img src="imgs/problem.png" alt="problem" title="problem" width="700">
</p>

<p align="justify">
  <strong> Multimodal embedding distributions across various distillation methods </strong>:
  We extract image and text embeddings from a finetuned CLIP and project them into a shared representation space using DOSNES. 
  Red triangles and blue circles denote image and text embeddings, respectively. 
  Left: Embeddings from randomly sampled data in the original dataset exhibit a well-spread and modality-aligned distribution. 
  Middle: The distilled dataset generated by a sota MDD method (LoRS) leads to Modality Collapse, where image and text embeddings are poorly aligned and concentrated in distinct regions. 
  Right: Our method effectively mitigates modality collapse, yielding a distribution that better preserves cross-modal alignment and exhibits greater representational diversity.
</p>
---
---

## ‚öôÔ∏è Installation

To get started, follow these instructions to set up the environment and install dependencies.

1. **Clone this repository**:
    ```bash
    git clone https://github.com/zhangxin-xd/RepBlend.git
    cd RepBlend
    ```

2. **Install required packages**:
    ```
    conda create -n RepBlend python=3.10
    conda activate RepBlend
    pip install -r requirements.txt
    ```
---

## üöÄ Usage

Here‚Äôs how to use RepBlend for Multimodal Dataset Distillation:
### Pretrained Weights
The checkpoints for all experimental networks are available from their respective official repositories. For convenience, we have also provided them together [here](https://drive.google.com/drive/folders/1FVwpyANNMWMEvM8X5pLYQzgPoblqkmXh?usp=sharing).
Once downloaded, put them in `distill_utils/checkpoints/`.

### Experimental Datasets
The dataset hase been validated on various benchmarks, you can download from  their respective links. Once downloaded, put them in `distill_utils/data/`.
| datasets | links| 
|-----|-----|
| Flickr30K | [images](https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset), [annotations](https://drive.google.com/drive/folders/1qj3Se9GqalufkrhU0qN2KOMob5eyGkAc?usp=drive_link)|
| COCO | [images](https://cocodataset.org/#download), [annotations](https://drive.google.com/drive/folders/1fnUHCyRsvsc27LPPoWxP4XvEiqZTV4hj?usp=sharing) |
|LLaVA-cc3m|[images](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md), [annotations](https://drive.google.com/drive/folders/1b8S0sJmyqRaAioL9oZGeIW8_EPx0iAzi?usp=sharing)|

### Generate Expert Buffer
You can 




- **Preparation**Ôºö
    For ImageNet-1K, we use the pre-trained weights available in `torchvision`.  For CIFAR and Tiny-ImageNet, we provide the trained weights at this [link](https://drive.google.com/drive/folders/1dH96COYa4kCquQ4c6wEnt7QobGMl6M3N?usp=sharing).  Alternatively, you can train the models yourself by following the instructions in [Diversity-Driven-Synthesis](https://github.com/AngusDujw/Diversity-Driven-Synthesis).

- **Generation**:
    Before performing distillation, please first prepare the images by randomly sampling from the original dataset and saving them as tensors. We provide the tensor-formatted initialization images at this [link](https://drive.google.com/drive/folders/1ueAnTXOUGiQ_E9iIssNYmEBX4vlVQEDZ?usp=sharing) .

    Cifar:
    ```bash
    python ufc_generation/ufc_cifar.py \
        --iteration 1000 --r-bn 1 --batch-size 100 \
        --lr 0.25 --ipc 10 \
        --exp-name generated_results \
        --wandb-name cifar100-ipc10 \
        --store-best-images \
        --syn-data-path syn/ \
        --init_path init_images/c100/ \
        --dataset cifar100
    ```
    ImageNet-1K:
    ```bash
    python ufc_generation/ufc_imgnet.py \
        --iteration 2000 --r-bn 0.01 --batch-size 1000 \
        --lr 0.25 --ipc 10 \
        --exp-name generated_results \
        --wandb-name imagenet-ipc10 \
        --store-best-images \
        --syn-data-path syn/ \
        --init_path init_images/imagenet/ \
        --dataset imagenet
    ```
- **Evaluation**:
  
  validation with static labeling
    ```bash
    python ufc_validation/val_static.py \
        --epochs 400 --batch-size 64 --ipc 10 \
        --syn-data-path syn/cifar100-ipc10/generated_results \
        --output-dir syn/cifar100-ipc10/generated_results \
        --wandb-name cifar100-ipc10 \
        --dataset cifar100 --networks resnet18
    ```
    
  validation with dynamic labeling
  
  Note: the number of training epochs is reduced by a factor of 1/(M + 1).
  ```bash
    python ufc_validation/val_dyn.py \
        --epochs 80 --batch-size 64 --ipc 10 \
        --syn-data-path syn/cifar100-ipc10/generated_results\
        --output-dir syn/cifar100-ipc10 \
        --wandb-name cifar100-ipc10 \
        --dataset cifar100 --networks resnet18
    ```

we also provide the `.sh` script in the `sh` directory.

---

## üìä Results

Our experiments demonstrate the effectiveness of the proposed approach across various benchmarks. 
<div style="display: flex; justify-content: center; align-items: center;">
    <img src="imgs/results 1.png" alt="Results 1" width="500"/>
</div>
<br>
<div style="display: flex; justify-content: center; align-items: center;">
    <img src="imgs/table 1.png" alt="table 1" width="250"/>
    <img src="imgs/table 2.png" alt="table 2" width="250"/>
</div>

For detailed experimental results and further analysis, please refer to the full paper.

---

## üìë Citation

If you find this code useful in your research, please consider citing our work:

```bibtex
@inproceedings{RepBlend2025neurips,
    title={Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation},
    author={Zhang, Xin and Zhang, Ziruo, and Du, Jiawei and Liu, Zuozhu and Zhou, Joey Tianyi},
    booktitle={Adv. Neural Inf. Process. Syst. (NeurIPS)},
    year={2025}
}
```
---
## üéâ Reference
Our code has referred to previous works:
- [LoRS: Low-Rank Similarity Mining](https://github.com/silicx/LoRS_Distill)
- [Vision-Language Dataset Distillation](https://github.com/princetonvisualai/multimodal_dataset_distillation)
- [Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory (TESLA)](https://github.com/justincui03/tesla)

